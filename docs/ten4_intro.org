#+title: tensorForth
#+subtitle: Forth lives in GPU
#+OPTIONS: toc:nil num:nil html-postamble:nil ^:{} reveal_title_slide:nil
#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
#+REVEAL_THEME: night
#+REVEAL_HLEVEL: 2
#+REVEAL_EXTRA_CSS: ./org-reveal.css
#+REVEAL_INIT_OPTIONS: slideNumber:"c/t", transition:"none", transitionSpeed:"fast", controlsTutorial:false, minScale:1.0, maxScale:1.5
#+REVEAL_EXTRA_SCRIPT: for(let e of document.getElementsByClassName("figure-number")){e.parentElement.classList.add("fig-caption");}
#+REVEAL_TITLE_SLIDE: <h2 class="title">%t</h2><em>%s</em><br><br>%a<br>%d<br>
#+REVEAL_PLUGINS: highlight notes
#+MACRO: color @@html:<font color="$1">$2</font>@@

* Artificial Intelligence
+ Neural Network, not Expert System
  #+ATTR_REVEAL: :frag roll-in
  - Matrix elements as neurons
  - Many layers, thus "Deep" Learning
+ Image/Video Processing
  #+ATTR_REVEAL: :frag roll-in
  - CNN (Convectional Neural Network)
  - think of it as eyes
+ Natural Language Processing
  #+ATTR_REVEAL: :frag roll-in
  - RNN (Recurrent NN)
  - Transformers (does better than RNN)
  - think of it as hearing & speech
#+BEGIN_NOTES
this is only a tests
#+END_NOTES
* Tensor
#+ATTR_REVEAL: :frag roll-in
+ Kept as a multi-dimensional array of numbers
+ Is a multi-linear map between vector spaces
+ Can embed reference vectors (on this later)
+ Generalized with rank
  - Scalar - rank 0
  - Vector - rank 1
  - Matrix - rank 2
  - 3D Array - rank 3
  - Higher order - rank 4, 5, ...
* GPU fuels AI frenzy
+ With its
  - Massive parallelism
  - High thoughput
  - Local Memory
  - Fast math hardware, and
  - Tensor Cores for matrix ops
** What is GPU anyway
+ Graphics Processing Unit
  #+ATTR_REVEAL: :frag roll-in
  - A specialized processor
  - Many parallel cores
  - Did graphics & games only
+ Becomes General Purpose
  #+ATTR_REVEAL: :frag roll-in
  - Talk to CPU via PCIe
  - Good for scientific computing
  - Machine learning, of course
+ nVidia {{{color(red,98%)}}}, AMD ~2%, Intel < 1% (data center)
** Programming nVidia GPU
+ CUDA - C/C++ API
  #+ATTR_REVEAL: :frag roll-in
  - A LLVM/gcc variant
  - Translate C++ to PTX (portable code)
  - Assemble PTX to SASS (device dependent)
+ Python - dominant language for AI
  #+ATTR_REVEAL: :frag roll-in
  - PyCUDA - transcoder to C/C++
  - NumPy - an optimized array library
+ High level frameworks
  #+ATTR_REVEAL: :frag roll-in
  - PyTorch, TensorFlow, ...
  - Backend utilize CUDA libraries
  - Encapsulate CUDA calls
* Forth in GPU?
#+ATTR_REVEAL: :frag roll-in
+ Similar to a DSP? or MCU?
+ In assembly, C++, or Forth?
+ Data types, float vs int?
+ Data Structures?
+ Big stack or memory allocation?
+ CPU-GPU dataflow?
+ IO handling?
* Forth in GPU? - cont'
+ Compile/Run vs Interpret?
+ Code on host or device?
+ Memory types & transfer?
+ Parallel & dimensions?
+ Threading & divergence?
+ Streaming & workload?
* tensorForth
+ tensor + Forth = tensorForth
#+ATTR_REVEAL: :frag roll-in
+ 100% C/C++-based + CUDA
  #+ATTR_REVEAL: :frag roll-in
  - Dictionary in device
  - Host input to device
  - Device output to host
+ Built-in Vocabularies
  #+ATTR_REVEAL: :frag roll-in
  - Linear Algebra
  - Machine Learning
  - Dataset Loader
+ Export for sharing & visualization
* Considerations
#+ATTR_REVEAL: :frag roll-in
+ Float data only? Yes, 32-bit!
+ Host or kernel libraries? Kernel!
+ number of data stacks? Only one!
  - Mix data & objects
+ Dynamic Memory allocation? TLSF!
  - reference counting
  - used & free queues
+ Async IO? Yes!
  - via message queue
  - external dataset loader
* Example - GEMM
- Multiply large matrices
#+begin_src
512 1024 matrix rand      \ create a 512x1024 matrix with random values
1024 256 matrix ones      \ create a 1024x256 matrix filled with 1s
@                         \ multiply the matrices
1024 /= .                 \ scale down element-wise and print

: mx                      \ create a word for benchmark loops
  1- dup >r clock >r      \ keep loop count and init clock on return stack
  for @ drop next         \ loop of matrix multiplication (and drop the result)
  clock r> -              \ time it (clock1 - clock0)
  r> 1 + / ." =>"         \ retrieve loop count and calc average
  . ."  msec/cycle" cr ;  \ print result
see mx                    \ show the word

100 mx                    \ run the multiplication loop 100 times
#+end_src
* Analysis - GEMM
#+ATTR_REVEAL: :frag roll-in
+ Forthy syntax works nicely
+ Polymorphic words
  - +,-,*,/ work on both arrays and numbers
  - @ to fetch or multiply matrices
+ Matrices kept warm on device
  - No CPU-GPU shuffling
+ Array/Matrix data structure needed
+ Temporary storage needed
+ Soft view needed (vs hard copy)
+ Memory managed dynamically
* Example - CNN Models
+ MNIST model definitions
#+begin_src
: model_a                       \ A model template
  0.5 10 conv2d                 \ 1st 2D convolution layer
  2 maxpool relu ;              \ with maxpool and relu activation
: model_b                       \ B model template
  0.5 10 conv2d 0.5 dropout     \ 1st 2D conv with dropout
  flatten 100 linear relu ;     \ linear connection with relu activation

50 28 28 1 nn.model             \ create a model (50 per batch of 28x28x1 img)
model_a                         \ choose model_a for tests
10 linear softmax               \ add final fully connected layer
dup constant md0                \ keep model as a constant

batchsize dataset mnist_train   \ create MNIST dataset with model batch size
constant ds0                    \ keep dataset in a constant
#+end_src
* Cont' - CNN Training
#+begin_src
variable hit 0 hit !            \ create var for hit counter, and zero it
variable lox                    \ create var for epoch latest loss
0.001 constant lr               \ init learning rate (for Adam)
: epoch ( N ds -- N' )          \ one epoch thru entire dataset
  for                           \ fetch a mini-batch
    forward                     \ neural network forward pass
    loss.ce lox ! nn.hit hit +! \ collect latest loss and accumulate hit
    backprop                    \ neural network back propegation
    lr nn.adam                  \ train with Adam Gradient Descent (b1=0.9,b2=0.999)
    46 emit
  next ;
: cnn ( N ds n -- N' ) 1-       \ run multiple epochs
  for
    epoch r@ stat               \ run one epoch, display statistics
    lr 0.9 * [to] lr            \ decay learning rate
    ds0 rewind                  \ rewind entire dataset 
  next ;

ds0                             \ push dataset as TOS
20 cnn                          \ execute 20 epoches
#+end_src
* Analysis - CNN
#+ATTR_REVEAL: :frag roll-in
+ NN layers fit right on Forth stack
+ Training sequence reads natually
  - feed forward
  - calculate loss
  - back propegate
  - gradient descent
+ Tensor data structure needed
+ Polymorphic words, again
  - Iterate with for..next, do..loop
  - Access Dataset with r@
+ Vast dump, needs good visulization
* Thank you!
+ More to come soon...
[[https://raw.githubusercontent.com/chochain/tensorForth/master/docs/img/ten4_l7_loss.png]]



