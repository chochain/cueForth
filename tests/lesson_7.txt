\ GAN demo - MNIST
100 constant N                                \ mini-batch sample count
N dataset mnist_test constant ds0             \ create dataset MNIST (Real data)

: SEED N 128 1 1 tensor randn ;               \ create randn avg=0,std=1 input for fake data
: REAL N   1 1 1 tensor ones ;                \ create a real label tensor
: FAKE N   1 1 1 tensor zeros ;               \ create a fake label tensor

\ discriminator
: dn linear 0.2 leakyrelu 0.3 dropout ;       \ discriminator network layers
N 28 28 1 nn.model                            \ define Discriminator model (13-layer)
1024 dn 512 dn 256 dn                         \ with 3 linear blocks
1 linear sigmoid constant D                   \ as D, tanh (sigmoid = binary classification)

\ generator
: gn linear 0.2 leakyrelu ;                   \ generator network layers
N 128 1 1 nn.model                            \ define Generator Model (10-layer)
256 gn 512 gn 1024 gn                         \ with 3 linear blocks
784 linear tanh constant G                    \ as G, tanh result in [-1, 1)

\ statistics
variable _gr                                  \ G loss from fake samples
variable _dr                                  \ D loss for real samples
variable _df                                  \ D loss for fake samples
: stat ( -- ) cr                              \ display loss statistics
  ." t=" clock 1000 / int .
  ." , G="  _gr @ .
  ." , Dr=" _dr @ .
  ." , Df=" _df @ . cr mstat ;
  
\ fake data producer
: gen ( G D -- G D t ) swap                   \ generate a mini-batch of fake images
  SEED forward -1 n@                          \ from random seeds
  N 28 28 1 reshape4
  rot swap ;

\ optimizers
0.0001 constant lr                            \ init learning rate
: d_opti backprop lr 0.9 nn.adam ;            \ D backprop (with SGD optimization)
: g_opti backprop lr 0.9 nn.adam ;            \ G backprop (with Adam optimization)

\ output file name handlers
variable idx 0 idx !                          \ filename index number
: fn s" ../out/l7_0000" ;                     \ filename storage
fn + 1 - constant pos                         \ pointer to filename digits
: i2n                                         \ convert index number to ASCII digit
  begin dup 0> while
    swap 10 / int swap 1 -
  repeat
  drop 10 mod 48 + ;
: nxfn idx @                                  \ create next filename
  3 for dup r@ i2n pos r@ - c! next drop      \ translate index into digits
  1 idx +! fn ;                               \ increment index, return filename

\ main
: epoch ( G D ds -- G D )                     \ one epoch for the given data source
  for                                         \ loop thru mini-batches (on rs)
    forward REAL loss.bce _dr ! REAL backprop \ feed D with real data (from rs)
    gen ( G D t )                             \ G creates fake images
    forward FAKE loss.bce _df ! FAKE d_opti   \ feed D with fake data, update D
    0 trainable                               \ make D non-trainable
    gen ( G D t )                             \ recreate fake images (could reuse)
    forward REAL loss.bce _gr ! REAL backprop \ feed D as real (but without updating)
    1 trainable                               \ restore D as trainable
    1 n@ rot swap ( D G t ) g_opti            \ fetch D backprop image, feed as gLoss
    swap ( G D ) 46 emit                      \ display progress, restore D trainable
  next ;                                      \ loop, if next mini-batch available
: gan ( G D ds n -- G D )
  for
    epoch stat
    lr 0.95 * [to] lr                         \ learning rate decay 
    gen nxfn bin save drop                    \ create fake image dump
    ds0 rewind
  next drop ;
\ expected loss G=>low, Dr=>low, Df=>high (thinking fake as real)
G D ds0 99 gan

bye

