.( ## Convolution NN tests ## ) cr
1 trace
2 10 10 1 nn.model                \ create a model with NHWC tensor input
network                           \ display it

.( #### define seq1 ) cr
0.5 2 conv2d                      \ 2d convolution with bias=0.5, 10 output channel
2 maxpool                         \ 2x2 downsampling
relu                              \ ReLU activation

.( #### define seq2 ) cr
0.5 2 conv2d                      \ second 2d convolution
0.5 dropout                       \ drop out 50% of channels
2 maxpool                         \ 2x2 downsampling
relu                              \ ReLU activation

.( #### define lin1 ) cr
flatten                           \ flatten for dense layer (no need)
0.0 49 linear                     \ linearize to 50 output with no bias

.( #### define lin2 ) cr
0.5 dropout                       \ another 50% drop out
0.0 4 linear                      \ linerize to 4 output with no bias
softmax                           \ translate to probability

network                           \ display network model

.( #### save model as a constant ) cr
constant mnist                    \ save model as a constant
mnist                             \ retrieve model
network                           \ display the model

.( #### model feed foward ) cr
mstat
2 10 10 1 tensor eye 0.5 *=       \ create input image (random)
forward                           \ execute forward pass
mstat

.( #### fetch last layer i.e. output ) cr
-1 n@ . drop                      \ fetch forward result from model

.( #### calculate loss ) cr
2 4 matrix{ 0 0 1 0 0 1 0 0 }     \ create onehot vector
2 4 1 1 reshape4 .                \ reshape it into a labeled rank-4 tensor
constant 1hot                     \ store in a constant

1hot
loss.ce .                         \ calculate network loss

.( #### model back propagation ) cr
mstat
1hot
backprop
mstat

.( #### gradiant decent ) cr
0.001 0.9 nn.adam                 \ learn using Adam gradient descent
mstat

bye
